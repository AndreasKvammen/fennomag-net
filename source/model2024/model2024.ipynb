{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2be196c",
   "metadata": {},
   "source": [
    "# Magnetic Field Forecasting Neural Network (model2024)\n",
    "\n",
    "This notebook implements a dual-branch neural network for 15-minute forecasting of Earth's surface magnetic field variations. The model combines two data streams: (1) large-scale solar and geophysical parameters sampled at 15-minute intervals, and (2) high-resolution SECS-based current patterns sampled at 1-minute intervals. The architecture uses LSTM networks for temporal processing, convolutional layers for spatial feature extraction, and cross-attention mechanisms to fuse information across different spatial and temporal scales.\n",
    "\n",
    "Key Features:\n",
    "- 15-minute ahead forecasting of magnetic field components (Be, Bn, Bu)\n",
    "- Dual-branch architecture combining global and local information\n",
    "- Memory-efficient batch generation with time-shifted targets\n",
    "- Multi-scale temporal processing (24h and 3h lookback windows)\n",
    "- Cross-attention mechanism for feature fusion\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Setup and Imports\n",
    "    * Import required libraries\n",
    "    * Configure GPU settings\n",
    "    * Set random seeds\n",
    "    * Define data paths and constants\n",
    "\n",
    "2. Data Loading and Exploration\n",
    "- 2.1 Target Data\n",
    "    * Load magnetic field components (Be, Bn, Bu)\n",
    "    * Process timestamps\n",
    "    * Analyze data distribution\n",
    "- 2.2 Branch 1: Large-scale Data\n",
    "    * Load solar/geophysical parameters (15-min resolution)\n",
    "    * Organize feature groups (auroral, solar, seasonal, etc.)\n",
    "    * Validate data quality and timestamps\n",
    "- 2.3 Branch 2: SECS Data\n",
    "    * Load SECS grid data (1-min resolution)\n",
    "    * Verify spatial dimensions (21x21x3)\n",
    "    * Check temporal alignment\n",
    "\n",
    "3. Data Pipeline\n",
    "    * Initialize BatchGenerator with 15-min forecast horizon\n",
    "    * Create time-shifted training windows\n",
    "    * Implement efficient batch generation\n",
    "    * Split data (70/15/15)\n",
    "    * Create TensorFlow datasets\n",
    "\n",
    "4. Model Architecture\n",
    "- 4.1 Branch 1: LSTM Network\n",
    "    * Process 24h of 15-min resolution data\n",
    "    * Extract temporal features from global parameters\n",
    "- 4.2 Branch 2: CNN-LSTM Network\n",
    "    * Process 3h of 1-min resolution SECS data\n",
    "    * Extract spatiotemporal features\n",
    "- 4.3 Cross-Attention Fusion\n",
    "    * Multi-head attention for feature interaction\n",
    "    * Combine multi-scale information\n",
    "- 4.4 Decoder\n",
    "    * Dense layers for feature processing\n",
    "    * Output layer for 15-min ahead predictions\n",
    "\n",
    "5. Training\n",
    "    * Define custom loss function\n",
    "    * Configure Adam optimizer\n",
    "    * Set up callbacks (LR scheduling, early stopping)\n",
    "    * Monitor training progress\n",
    "    * Save best model\n",
    "\n",
    "6. Evaluation\n",
    "    * Calculate prediction metrics (MSE, RMSE)\n",
    "    * Analyze forecast accuracy\n",
    "    * Assess temporal performance\n",
    "    * Compare with baseline models\n",
    "\n",
    "7. Visualization\n",
    "    * Plot predicted vs actual values\n",
    "    * Visualize attention patterns\n",
    "    * Display component-wise performance\n",
    "    * Show error distributions\n",
    "    * Create forecast examples\n",
    "\n",
    "The model aims to provide accurate 15-minute forecasts of magnetic field variations by leveraging both long-term global patterns and short-term local dynamics. The dual-branch architecture and cross-attention mechanism enable the model to learn relevant features at different spatial and temporal scales while maintaining computational efficiency through careful batch generation and data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d2986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU memory growth enabled\n",
      "\n",
      "Model Configuration:\n",
      "Branch 1 Lookback: 96 timesteps (24 hours)\n",
      "Branch 2 Lookback: 180 timesteps (3 hours)\n",
      "Forecast Horizon: 15 minutes\n",
      "Batch Size: 32\n",
      "Embedding Dimension: 64\n",
      "Number of Attention Heads: 4\n",
      "Dropout Rate: 0.2\n",
      "\n",
      "Verifying data files:\n",
      "✓ Found: /Users/akv020/Tensorflow/fennomag-net/source/model2024/data/target.csv\n",
      "✓ Found: /Users/akv020/Tensorflow/fennomag-net/source/model2024/data/geodata.csv\n",
      "✓ Found: /Users/akv020/Tensorflow/fennomag-net/source/model2024/data/secs_data.npy\n",
      "✓ Found: /Users/akv020/Tensorflow/fennomag-net/source/model2024/data/secs_timestamps.npy\n",
      "\n",
      "Model configuration saved to: model_outputs/model_config.json\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and Imports\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, Model\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability and configure\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = \"/Users/akv020/Tensorflow/fennomag-net/source/model2024/data\"\n",
    "TARGET_PATH = os.path.join(DATA_DIR, \"target.csv\")\n",
    "GEODATA_PATH = os.path.join(DATA_DIR, \"geodata.csv\")\n",
    "SECS_DATA_PATH = os.path.join(DATA_DIR, \"secs_data.npy\")\n",
    "SECS_TIMESTAMPS_PATH = os.path.join(DATA_DIR, \"secs_timestamps.npy\")\n",
    "\n",
    "# Create output directories for model artifacts\n",
    "OUTPUT_DIR = Path(\"model_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "LOG_DIR = OUTPUT_DIR / \"logs\"\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "# Temporal parameters\n",
    "BRANCH1_LOOKBACK = 96    # 24 hours at 15-minute intervals\n",
    "BRANCH2_LOOKBACK = 180   # 3 hours at 1-minute intervals\n",
    "FORECAST_HORIZON = 15    # 15-minute forecast horizon\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Model architecture parameters\n",
    "EMBEDDING_DIM = 64       # Dimension of feature embeddings\n",
    "NUM_HEADS = 4           # Number of attention heads\n",
    "DROPOUT_RATE = 0.2      # Dropout rate for regularization\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "MIN_LR = 1e-6\n",
    "PATIENCE = 10           # Early stopping patience\n",
    "\n",
    "# Print configuration\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"Branch 1 Lookback: {BRANCH1_LOOKBACK} timesteps ({BRANCH1_LOOKBACK//4} hours)\")\n",
    "print(f\"Branch 2 Lookback: {BRANCH2_LOOKBACK} timesteps ({BRANCH2_LOOKBACK//60} hours)\")\n",
    "print(f\"Forecast Horizon: {FORECAST_HORIZON} minutes\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"Number of Attention Heads: {NUM_HEADS}\")\n",
    "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "\n",
    "# Verify data files exist\n",
    "print(\"\\nVerifying data files:\")\n",
    "for path in [TARGET_PATH, GEODATA_PATH, SECS_DATA_PATH, SECS_TIMESTAMPS_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required data file not found: {path}\")\n",
    "    else:\n",
    "        print(f\"✓ Found: {path}\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['figure.titlesize'] = 16\n",
    "\n",
    "# Set up color scheme for consistent visualization\n",
    "COLORS = {\n",
    "    'Be': '#1f77b4',  # Blue\n",
    "    'Bn': '#2ca02c',  # Green\n",
    "    'Bu': '#ff7f0e',  # Orange\n",
    "    'train': '#1f77b4',\n",
    "    'val': '#ff7f0e',\n",
    "    'test': '#2ca02c'\n",
    "}\n",
    "\n",
    "# Function to save model configuration\n",
    "def save_config():\n",
    "    \"\"\"Save model configuration to JSON file.\"\"\"\n",
    "    config = {\n",
    "        'temporal_params': {\n",
    "            'branch1_lookback': BRANCH1_LOOKBACK,\n",
    "            'branch2_lookback': BRANCH2_LOOKBACK,\n",
    "            'forecast_horizon': FORECAST_HORIZON,\n",
    "            'batch_size': BATCH_SIZE\n",
    "        },\n",
    "        'model_params': {\n",
    "            'embedding_dim': EMBEDDING_DIM,\n",
    "            'num_heads': NUM_HEADS,\n",
    "            'dropout_rate': DROPOUT_RATE\n",
    "        },\n",
    "        'training_params': {\n",
    "            'epochs': EPOCHS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'min_lr': MIN_LR,\n",
    "            'patience': PATIENCE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_DIR / 'model_config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(\"\\nModel configuration saved to:\", OUTPUT_DIR / 'model_config.json')\n",
    "\n",
    "# Save initial configuration\n",
    "save_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loading and Exploration\n",
    "\n",
    "# 2.1 Target Data\n",
    "print(\"Loading target data...\")\n",
    "target_df = pd.read_csv(TARGET_PATH)\n",
    "target_df['DateTime'] = pd.to_datetime(target_df['DateTime'])\n",
    "print(f\"Target data shape: {target_df.shape}\")\n",
    "print(\"\\nFirst few rows of target data:\")\n",
    "print(target_df.head())\n",
    "print(\"\\nTarget data info:\")\n",
    "print(target_df.info())\n",
    "\n",
    "# Calculate basic statistics for target components\n",
    "print(\"\\nTarget data statistics:\")\n",
    "print(target_df[['Be_0_0', 'Bn_0_0', 'Bu_0_0']].describe())\n",
    "\n",
    "# 2.2 Branch 1: Large-scale Data\n",
    "print(\"\\nLoading Branch 1 (large-scale) data...\")\n",
    "geo_df = pd.read_csv(GEODATA_PATH)\n",
    "geo_df['DateTime'] = pd.to_datetime(geo_df['DateTime'])\n",
    "\n",
    "# Create feature groups for better organization\n",
    "feature_groups = {\n",
    "    'auroral': ['SME', 'SML', 'SMU'],\n",
    "    'ring_current': ['SYM_D', 'SYM_H'],\n",
    "    'disturbance': ['ASY_D', 'ASY_H'],\n",
    "    'solar': ['Sunspot', 'f107', 'ap_index', 'Lyman'],\n",
    "    'seasonal': ['DOY_cos', 'DOY_sin', 'TOD_cos', 'TOD_sin'],\n",
    "    'geometry': ['SolarZenithAngle'],\n",
    "    'previous_magnetic': ['BE', 'BN', 'BU'],\n",
    "    'magnetic_std': ['stdE', 'stdN', 'stdU']\n",
    "}\n",
    "\n",
    "print(f\"Branch 1 data shape: {geo_df.shape}\")\n",
    "print(\"\\nFeature groups:\")\n",
    "for group, features in feature_groups.items():\n",
    "    print(f\"{group}: {features}\")\n",
    "\n",
    "# Calculate sampling intervals\n",
    "branch1_interval = (geo_df['DateTime'].max() - geo_df['DateTime'].min()) / len(geo_df)\n",
    "print(f\"\\nBranch 1 sampling interval: {branch1_interval}\")\n",
    "\n",
    "# 2.3 Branch 2: SECS Data\n",
    "print(\"\\nLoading Branch 2 (SECS) data...\")\n",
    "secs_data = np.load(SECS_DATA_PATH)\n",
    "secs_timestamps = np.load(SECS_TIMESTAMPS_PATH)\n",
    "secs_timestamps = pd.to_datetime(secs_timestamps)\n",
    "\n",
    "print(f\"SECS data shape: {secs_data.shape}\")\n",
    "print(f\"Number of SECS timestamps: {len(secs_timestamps)}\")\n",
    "\n",
    "# Calculate sampling intervals\n",
    "branch2_interval = (secs_timestamps.max() - secs_timestamps.min()) / len(secs_timestamps)\n",
    "print(f\"Branch 2 sampling interval: {branch2_interval}\")\n",
    "\n",
    "# Basic data validation\n",
    "print(\"\\nData validation:\")\n",
    "print(f\"Target data time range: {target_df['DateTime'].min()} to {target_df['DateTime'].max()}\")\n",
    "print(f\"Branch 1 time range: {geo_df['DateTime'].min()} to {geo_df['DateTime'].max()}\")\n",
    "print(f\"Branch 2 time range: {secs_timestamps.min()} to {secs_timestamps.max()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values check:\")\n",
    "print(\"Target data missing values:\")\n",
    "print(target_df.isnull().sum())\n",
    "print(\"\\nBranch 1 missing values:\")\n",
    "print(geo_df.isnull().sum())\n",
    "\n",
    "# Visualize data distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Target data distribution\n",
    "plt.subplot(221)\n",
    "for col, color in zip(['Be_0_0', 'Bn_0_0', 'Bu_0_0'], [COLORS['Be'], COLORS['Bn'], COLORS['Bu']]):\n",
    "    plt.hist(target_df[col], bins=50, alpha=0.5, label=col, color=color)\n",
    "plt.title('Target Data Distribution')\n",
    "plt.xlabel('Magnetic Field Component Value')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Branch 1 feature distribution (auroral indices)\n",
    "plt.subplot(222)\n",
    "for col in feature_groups['auroral']:\n",
    "    plt.hist(geo_df[col], bins=50, alpha=0.5, label=col)\n",
    "plt.title('Auroral Indices Distribution')\n",
    "plt.xlabel('Index Value')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Branch 1 feature distribution (solar parameters)\n",
    "plt.subplot(223)\n",
    "for col in feature_groups['solar']:\n",
    "    plt.hist(geo_df[col], bins=50, alpha=0.5, label=col)\n",
    "plt.title('Solar Parameters Distribution')\n",
    "plt.xlabel('Parameter Value')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Branch 2 data distribution\n",
    "plt.subplot(224)\n",
    "plt.hist(secs_data.ravel(), bins=50, alpha=0.5, color='blue')\n",
    "plt.title('SECS Data Distribution')\n",
    "plt.xlabel('Current Density Value')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional temporal analysis\n",
    "print(\"\\nTemporal Analysis:\")\n",
    "print(f\"Total time span: {target_df['DateTime'].max() - target_df['DateTime'].min()}\")\n",
    "print(f\"Number of days: {(target_df['DateTime'].max() - target_df['DateTime'].min()).days}\")\n",
    "print(f\"Average samples per day: {len(target_df) / ((target_df['DateTime'].max() - target_df['DateTime'].min()).days):.2f}\")\n",
    "\n",
    "# Save data statistics\n",
    "data_stats = {\n",
    "    'target_stats': target_df[['Be_0_0', 'Bn_0_0', 'Bu_0_0']].describe().to_dict(),\n",
    "    'temporal_info': {\n",
    "        'total_days': (target_df['DateTime'].max() - target_df['DateTime'].min()).days,\n",
    "        'samples_per_day': len(target_df) / ((target_df['DateTime'].max() - target_df['DateTime'].min()).days),\n",
    "        'branch1_interval': str(branch1_interval),\n",
    "        'branch2_interval': str(branch2_interval)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'data_statistics.json', 'w') as f:\n",
    "    json.dump(data_stats, f, indent=4)\n",
    "print(\"\\nData statistics saved to:\", OUTPUT_DIR / 'data_statistics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38412361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Pipeline\n",
    "\n",
    "print(\"\\nInitializing Data Pipeline...\")\n",
    "print(f\"Forecast Horizon: {FORECAST_HORIZON} minutes\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Train/Val/Test Split: 70/15/15\")\n",
    "\n",
    "# Import the BatchGenerator\n",
    "from batch_generator import BatchGenerator\n",
    "\n",
    "# Calculate valid observation windows\n",
    "valid_start = pd.Timestamp('2024-01-02 00:00:00')  # First valid observation\n",
    "valid_end = pd.Timestamp('2024-12-31 23:30:00')    # Last valid observation\n",
    "\n",
    "print(\"\\nValid Observation Windows:\")\n",
    "print(f\"Start of valid observations: {valid_start}\")\n",
    "print(f\"End of valid observations: {valid_end}\")\n",
    "print(f\"Total valid observation period: {(valid_end - valid_start).total_seconds() / 3600:.1f} hours\")\n",
    "\n",
    "# Initialize the batch generator with time-shifted targets\n",
    "print(\"\\nInitializing BatchGenerator...\")\n",
    "batch_generator = BatchGenerator(\n",
    "    target_path=TARGET_PATH,\n",
    "    geodata_path=GEODATA_PATH,\n",
    "    secs_data_path=SECS_DATA_PATH,\n",
    "    secs_timestamps_path=SECS_TIMESTAMPS_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    valid_start=valid_start,\n",
    "    valid_end=valid_end\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets for each split\n",
    "print(\"\\nCreating TensorFlow datasets...\")\n",
    "train_dataset = batch_generator.create_tf_dataset(split='train')\n",
    "val_dataset = batch_generator.create_tf_dataset(split='val')\n",
    "test_dataset = batch_generator.create_tf_dataset(split='test')\n",
    "\n",
    "# Verify dataset shapes and temporal alignment\n",
    "print(\"\\nVerifying dataset shapes and temporal alignment:\")\n",
    "for name, dataset in [('Training', train_dataset), ('Validation', val_dataset), ('Test', test_dataset)]:\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    for batch in dataset.take(1):\n",
    "        # Print shapes\n",
    "        print(f\"Branch 1 input shape: {batch['branch1_input'].shape}\")  # (batch_size, 96, n_features)\n",
    "        print(f\"Branch 2 input shape: {batch['branch2_input'].shape}\")  # (batch_size, 180, 21, 21, 3)\n",
    "        print(f\"Target shape: {batch['target'].shape}\")  # (batch_size, 3)\n",
    "        \n",
    "        # Verify temporal windows\n",
    "        print(\"\\nTemporal window verification:\")\n",
    "        print(\"Branch 1: 96 timesteps = 24 hours ending at current time t\")\n",
    "        print(\"Branch 2: 180 timesteps = 3 hours ending at current time t\")\n",
    "        print(\"Target: 15 minutes ahead of current time t\")\n",
    "        \n",
    "        # Get a sample from the batch\n",
    "        sample_idx = 0  # First sample in batch\n",
    "        \n",
    "        # Get the corresponding indices for the split\n",
    "        if name == 'Training':\n",
    "            indices = batch_generator.train_indices\n",
    "        elif name == 'Validation':\n",
    "            indices = batch_generator.val_indices\n",
    "        else:\n",
    "            indices = batch_generator.test_indices\n",
    "            \n",
    "        if len(indices) > 0:\n",
    "            # Get a valid index from the split\n",
    "            idx = indices[0]  # Use first index instead of random to ensure consistency\n",
    "            \n",
    "            # Get timestamps\n",
    "            current_time = batch_generator.target_df['DateTime'].iloc[idx]\n",
    "            target_time = current_time + pd.Timedelta(minutes=15)  # 15 minutes ahead\n",
    "            \n",
    "            print(\"\\nSample timestamps:\")\n",
    "            print(f\"Current time (t): {current_time}\")\n",
    "            print(f\"Branch 1 window: [{current_time - pd.Timedelta(hours=24)} ... {current_time}]\")\n",
    "            print(f\"Branch 2 window: [{current_time - pd.Timedelta(hours=3)} ... {current_time}]\")\n",
    "            print(f\"Target time: {target_time} (t + 15min)\")\n",
    "            \n",
    "            # Print sample values\n",
    "            print(\"\\nSample values:\")\n",
    "            print(\"Branch 1 input (first and last timestep):\")\n",
    "            print(\"First timestep (t-24h):\", batch['branch1_input'][sample_idx, 0, :5].numpy(), \"...\")\n",
    "            print(\"Last timestep (t):\", batch['branch1_input'][sample_idx, -1, :5].numpy(), \"...\")\n",
    "            \n",
    "            print(\"\\nBranch 2 input (first and last timestep, center pixel):\")\n",
    "            print(\"First timestep (t-3h):\", batch['branch2_input'][sample_idx, 0, 10, 10, :].numpy())\n",
    "            print(\"Last timestep (t):\", batch['branch2_input'][sample_idx, -1, 10, 10, :].numpy())\n",
    "            \n",
    "            print(\"\\nTarget values (t + 15min):\")\n",
    "            print(f\"Be: {batch['target'][sample_idx, 0].numpy():.2f}\")\n",
    "            print(f\"Bn: {batch['target'][sample_idx, 1].numpy():.2f}\")\n",
    "            print(f\"Bu: {batch['target'][sample_idx, 2].numpy():.2f}\")\n",
    "        break\n",
    "\n",
    "# Visualize temporal alignment\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot sample sequences with timestamps\n",
    "plt.subplot(131)\n",
    "sample_branch1 = batch['branch1_input'][0, :, 0].numpy()  # First feature over time\n",
    "plt.plot(sample_branch1, color=COLORS['train'], label='Branch 1')\n",
    "plt.axvline(x=95, color='red', linestyle='--', label='Current time (t)')\n",
    "plt.title('Branch 1 Sequence\\n(24 hours ending at t)')\n",
    "plt.xlabel('Timestep (15-min intervals)')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "sample_branch2 = batch['branch2_input'][0, :, 10, 10, 0].numpy()  # Center pixel, first channel\n",
    "plt.plot(sample_branch2, color=COLORS['train'], label='Branch 2')\n",
    "plt.axvline(x=179, color='red', linestyle='--', label='Current time (t)')\n",
    "plt.title('Branch 2 Sequence\\n(3 hours ending at t)')\n",
    "plt.xlabel('Timestep (1-min intervals)')\n",
    "plt.ylabel('Current Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "target_components = ['Be', 'Bn', 'Bu']\n",
    "for i, (comp, color) in enumerate(zip(target_components, [COLORS['Be'], COLORS['Bn'], COLORS['Bu']])):\n",
    "    plt.axhline(y=batch['target'][0, i].numpy(), color=color, label=comp, linestyle='--')\n",
    "plt.title('Target Values\\n(t + 15min)')\n",
    "plt.legend()\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Magnetic Field Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData pipeline setup complete. Ready for model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d766798",
   "metadata": {},
   "source": [
    "## 4.1 Branch 1: LSTM Network\n",
    "The first branch processes the large-scale solar and geophysical data using LSTM layers to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1 Branch 1: LSTM Network\n",
    "# The first branch processes the large-scale solar and geophysical data using LSTM layers to capture temporal dependencies.\n",
    "\n",
    "# Define model parameters\n",
    "EMBEDDING_DIM = 64  # Dimension of the embedding space\n",
    "branch1_shape = (96, 22)  # (timesteps, features) for Branch 1\n",
    "\n",
    "def create_branch1_model(input_shape, embedding_dim=64, dropout_rate=0.2):\n",
    "    \"\"\"Create the LSTM-based model for Branch 1 (large-scale data).\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of the input data (timesteps, features)\n",
    "        embedding_dim: Dimension of the embedding space\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        \n",
    "    Returns:\n",
    "        Keras model for Branch 1\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name='branch1_input')\n",
    "    \n",
    "    # First LSTM layer with return sequences\n",
    "    x = layers.LSTM(embedding_dim, return_sequences=True, \n",
    "                    name='branch1_lstm1')(inputs)\n",
    "    x = layers.BatchNormalization(name='branch1_bn1')(x)\n",
    "    x = layers.Dropout(dropout_rate, name='branch1_dropout1')(x)\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    x = layers.LSTM(embedding_dim, return_sequences=False, \n",
    "                    name='branch1_lstm2')(x)\n",
    "    x = layers.BatchNormalization(name='branch1_bn2')(x)\n",
    "    x = layers.Dropout(dropout_rate, name='branch1_dropout2')(x)\n",
    "    \n",
    "    # Dense layer to create embedding\n",
    "    outputs = layers.Dense(embedding_dim, activation='relu', \n",
    "                          name='branch1_embedding')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs, name='branch1_model')\n",
    "\n",
    "# Test Branch 1\n",
    "print(\"\\nTesting Branch 1:\")\n",
    "branch1_model = create_branch1_model(branch1_shape, EMBEDDING_DIM)\n",
    "branch1_model.summary()\n",
    "# Test with sample input\n",
    "sample_branch1 = tf.random.normal((1, *branch1_shape))\n",
    "branch1_output = branch1_model(sample_branch1)\n",
    "print(f\"Branch 1 output shape: {branch1_output.shape}\")  # Should be (1, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e6b39",
   "metadata": {},
   "source": [
    "## 4.2 Branch 2: CNN-LSTM Network\n",
    "The second branch processes the SECS grid data using a combination of convolutional and LSTM layers to capture both spatial and temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbdc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2 Branch 2: CNN-LSTM Network\n",
    "# The second branch processes the SECS grid data using a combination of convolutional and LSTM layers to capture both spatial and temporal patterns.\n",
    "\n",
    "# Define model parameters\n",
    "branch2_shape = (180, 21, 21, 3)  # (timesteps, height, width, channels) for Branch 2\n",
    "\n",
    "def create_branch2_model(input_shape, embedding_dim=64, dropout_rate=0.2):\n",
    "    \"\"\"Create the CNN-LSTM model for Branch 2 (SECS data).\n",
    "    \n",
    "    This model:\n",
    "    1. Processes each time step with Conv2D layers to extract spatial features\n",
    "    2. Flattens the spatial features\n",
    "    3. Feeds the flattened features through LSTM layers to capture temporal patterns\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of the input data (timesteps, height, width, channels)\n",
    "        embedding_dim: Dimension of the embedding space\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        \n",
    "    Returns:\n",
    "        Keras model for Branch 2\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name='branch2_input')\n",
    "    \n",
    "    # Process each time step with Conv2D layers\n",
    "    # We'll use a TimeDistributed wrapper to apply the same Conv2D operations to each time step\n",
    "    \n",
    "    # First Conv2D layer: 21x21 -> 19x19\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='valid'),\n",
    "        name='branch2_conv1'\n",
    "    )(inputs)\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.BatchNormalization(),\n",
    "        name='branch2_bn1'\n",
    "    )(x)\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        name='branch2_pool1'\n",
    "    )(x)  # 19x19 -> 9x9\n",
    "    \n",
    "    # Second Conv2D layer: 9x9 -> 7x7\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='valid'),\n",
    "        name='branch2_conv2'\n",
    "    )(x)\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.BatchNormalization(),\n",
    "        name='branch2_bn2'\n",
    "    )(x)\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        name='branch2_pool2'\n",
    "    )(x)  # 7x7 -> 3x3\n",
    "    \n",
    "    # Flatten the spatial dimensions for each time step\n",
    "    # Shape: (batch, timesteps, 3*3*32)\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.Flatten(),\n",
    "        name='branch2_flatten'\n",
    "    )(x)\n",
    "    \n",
    "    # Apply dropout to the flattened features\n",
    "    x = layers.Dropout(dropout_rate, name='branch2_dropout1')(x)\n",
    "    \n",
    "    # First LSTM layer with return sequences\n",
    "    x = layers.LSTM(embedding_dim, return_sequences=True, \n",
    "                    name='branch2_lstm1')(x)\n",
    "    x = layers.BatchNormalization(name='branch2_bn3')(x)\n",
    "    x = layers.Dropout(dropout_rate, name='branch2_dropout2')(x)\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    x = layers.LSTM(embedding_dim, return_sequences=False, \n",
    "                    name='branch2_lstm2')(x)\n",
    "    x = layers.BatchNormalization(name='branch2_bn4')(x)\n",
    "    x = layers.Dropout(dropout_rate, name='branch2_dropout3')(x)\n",
    "    \n",
    "    # Dense layer to create embedding\n",
    "    outputs = layers.Dense(embedding_dim, activation='relu', \n",
    "                          name='branch2_embedding')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs, name='branch2_model')\n",
    "\n",
    "print(\"\\nTesting Branch 2:\")\n",
    "branch2_model = create_branch2_model(branch2_shape, EMBEDDING_DIM)\n",
    "branch2_model.summary()\n",
    "# Test with sample input\n",
    "sample_branch2 = tf.random.normal((1, *branch2_shape))\n",
    "branch2_output = branch2_model(sample_branch2)\n",
    "print(f\"Branch 2 output shape: {branch2_output.shape}\")  # Should be (1, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417622",
   "metadata": {},
   "source": [
    "## 4.3 Cross-Attention Fusion\n",
    "The cross-attention mechanism allows the model to focus on the most relevant features from both branches when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(layers.Layer):\n",
    "    \"\"\"Custom layer implementing cross-attention between two feature vectors.\n",
    "    \n",
    "    This layer computes attention scores between two feature vectors and\n",
    "    produces a weighted combination of the second vector based on the first.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads=4, key_dim=16, **kwargs):\n",
    "        super(CrossAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Create projection matrices for query, key, and value\n",
    "        self.query_dense = layers.Dense(self.num_heads * self.key_dim)\n",
    "        self.key_dense = layers.Dense(self.num_heads * self.key_dim)\n",
    "        self.value_dense = layers.Dense(self.num_heads * self.key_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_dense = layers.Dense(input_shape[0][-1])\n",
    "        \n",
    "        super(CrossAttention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Unpack inputs\n",
    "        query, key_value = inputs\n",
    "        \n",
    "        # Project inputs to query, key, and value\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key_value)\n",
    "        value = self.value_dense(key_value)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        query = tf.reshape(query, [batch_size, 1, self.num_heads, self.key_dim])\n",
    "        key = tf.reshape(key, [batch_size, 1, self.num_heads, self.key_dim])\n",
    "        value = tf.reshape(value, [batch_size, 1, self.num_heads, self.key_dim])\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = tf.matmul(query, key, transpose_b=True)\n",
    "        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        attention_output = tf.matmul(attention_weights, value)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        attention_output = tf.reshape(attention_output, [batch_size, self.num_heads * self.key_dim])\n",
    "        output = self.output_dense(attention_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(CrossAttention, self).get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'key_dim': self.key_dim\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "# Test Cross-Attention\n",
    "print(\"\\nTesting Cross-Attention:\")\n",
    "cross_attention = CrossAttention(num_heads=4, key_dim=16)\n",
    "# Test with sample inputs\n",
    "attended_features = cross_attention([branch1_output, branch2_output])\n",
    "print(f\"Cross-attention output shape: {attended_features.shape}\")  # Should be (1, 64)\n",
    "\n",
    "# Test reverse direction\n",
    "attended_features_reverse = cross_attention([branch2_output, branch1_output])\n",
    "print(f\"Reverse cross-attention output shape: {attended_features_reverse.shape}\")  # Should be (1, 64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cb5c3",
   "metadata": {},
   "source": [
    "## 4.4 Decoder\n",
    "The decoder takes the fused features and produces the final predictions for the three magnetic field components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Decoder\n",
    "def create_decoder(embedding_dim, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Creates the decoder part of the model that produces final predictions.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim (int): Dimension of the input features (will be doubled due to concatenation)\n",
    "        dropout_rate (float): Dropout rate for regularization\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Decoder model\n",
    "    \"\"\"\n",
    "    # Note: input shape is 2*embedding_dim because we concatenate two vectors\n",
    "    inputs = layers.Input(shape=(2*embedding_dim,))\n",
    "    \n",
    "    # Dense layers with batch normalization and dropout\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output layer for 3 magnetic field components\n",
    "    outputs = layers.Dense(3, activation='linear', name='decoder_output')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# Test Decoder\n",
    "print(\"\\nTesting Decoder:\")\n",
    "decoder = create_decoder(EMBEDDING_DIM)\n",
    "decoder.summary()\n",
    "# Test with sample input (note: this should be 128-dimensional after concatenation)\n",
    "sample_fused = tf.random.normal((1, 128))  # Changed from 64 to 128\n",
    "decoder_output = decoder(sample_fused)\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")  # Should be (1, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99d572",
   "metadata": {},
   "source": [
    "## 4.5 Complete Model\n",
    "Now we'll combine all components to create the complete dual-branch neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e66744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.5 Complete Model\n",
    "# Now we'll combine all components to create the complete dual-branch neural network model.\n",
    "\n",
    "# Define additional model parameters\n",
    "NUM_HEADS = 4  # Number of attention heads\n",
    "DROPOUT_RATE = 0.2  # Dropout rate for regularization\n",
    "\n",
    "def create_dual_branch_model(branch1_shape, branch2_shape, embedding_dim=64, num_heads=4, dropout_rate=0.2):\n",
    "    \"\"\"Create the complete dual-branch neural network model.\n",
    "    \n",
    "    Args:\n",
    "        branch1_shape: Shape of Branch 1 input (timesteps, features)\n",
    "        branch2_shape: Shape of Branch 2 input (timesteps, height, width, channels)\n",
    "        embedding_dim: Dimension of the embedding space\n",
    "        num_heads: Number of attention heads\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        \n",
    "    Returns:\n",
    "        Complete Keras model\n",
    "    \"\"\"\n",
    "    # Create input layers\n",
    "    branch1_input = layers.Input(shape=branch1_shape, name='branch1_input')\n",
    "    branch2_input = layers.Input(shape=branch2_shape, name='branch2_input')\n",
    "    \n",
    "    # Create branch models\n",
    "    branch1_model = create_branch1_model(branch1_shape, embedding_dim, dropout_rate)\n",
    "    branch2_model = create_branch2_model(branch2_shape, embedding_dim, dropout_rate)\n",
    "    \n",
    "    # Process inputs through branch models\n",
    "    branch1_features = branch1_model(branch1_input)\n",
    "    branch2_features = branch2_model(branch2_input)\n",
    "    \n",
    "    # Apply cross-attention in both directions\n",
    "    branch1_attended = CrossAttention(num_heads=num_heads, key_dim=embedding_dim//num_heads, \n",
    "                                      name='branch1_attention')([branch1_features, branch2_features])\n",
    "    branch2_attended = CrossAttention(num_heads=num_heads, key_dim=embedding_dim//num_heads, \n",
    "                                      name='branch2_attention')([branch2_features, branch1_features])\n",
    "    \n",
    "    # Concatenate attended features\n",
    "    fused_features = layers.Concatenate(name='feature_fusion')([branch1_attended, branch2_attended])\n",
    "    \n",
    "    # Create and apply decoder\n",
    "    decoder = create_decoder(embedding_dim, dropout_rate)\n",
    "    outputs = decoder(fused_features)\n",
    "    \n",
    "    # Create and return the complete model\n",
    "    model = Model(inputs=[branch1_input, branch2_input], outputs=outputs, name='dual_branch_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test Complete Model\n",
    "print(\"\\nTesting Complete Model:\")\n",
    "model = create_dual_branch_model(\n",
    "    branch1_shape=branch1_shape,\n",
    "    branch2_shape=branch2_shape,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Test with sample inputs\n",
    "sample_output = model([sample_branch1, sample_branch2])\n",
    "print(f\"Complete model output shape: {sample_output.shape}\")  # Should be (1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31156aae",
   "metadata": {},
   "source": [
    "## 4.6 Model Summary and Visualization\n",
    "Let's create the model and visualize its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfc6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.6 Model Summary and Visualization\n",
    "# Let's create the model and visualize its architecture.\n",
    "\n",
    "# Get input shapes from the datasets\n",
    "sample_batch = next(iter(train_dataset))\n",
    "branch1_shape = sample_batch['branch1_input'].shape[1:]\n",
    "branch2_shape = sample_batch['branch2_input'].shape[1:]\n",
    "\n",
    "print(f\"Branch 1 input shape: {branch1_shape}\")\n",
    "print(f\"Branch 2 input shape: {branch2_shape}\")\n",
    "\n",
    "# Create the model\n",
    "model = create_dual_branch_model(\n",
    "    branch1_shape=branch1_shape,\n",
    "    branch2_shape=branch2_shape,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Visualize model architecture\n",
    "try:\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_model(model, to_file='model_architecture.png', show_shapes=True)\n",
    "    print(\"\\nModel architecture saved to 'model_architecture.png'\")\n",
    "except ImportError:\n",
    "    print(\"\\nCould not visualize model architecture. Install pydot and graphviz for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3d459",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Training\n",
    "# 5.1 Define Loss Functions\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function combining MSE for each magnetic field component\n",
    "    with additional weighting for vertical component (Bu) which is typically\n",
    "    more important for space weather applications.\n",
    "    \"\"\"\n",
    "    # Split predictions into components\n",
    "    be_pred, bn_pred, bu_pred = tf.unstack(y_pred, axis=1)\n",
    "    be_true, bn_true, bu_true = tf.unstack(y_true, axis=1)\n",
    "    \n",
    "    # Calculate MSE for each component\n",
    "    be_loss = tf.keras.losses.mean_squared_error(be_true, be_pred)\n",
    "    bn_loss = tf.keras.losses.mean_squared_error(bn_true, bn_pred)\n",
    "    bu_loss = tf.keras.losses.mean_squared_error(bu_true, bu_pred)\n",
    "    \n",
    "    # Weight vertical component equally\n",
    "    return be_loss + bn_loss + bu_loss\n",
    "\n",
    "# 5.2 Configure Optimizer with Gradient Clipping\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=5e-4,  # More conservative initial learning rate\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    clipnorm=1.0  # Add gradient clipping\n",
    ")\n",
    "\n",
    "# 5.3 Set up Callbacks\n",
    "# Learning rate scheduler with more patience\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=7,  # Increased patience\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping with increased patience\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,  # Increased patience\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model checkpointing\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# TensorBoard logging\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "# 5.4 Compile Model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=custom_loss,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MeanSquaredError(name='mse'),\n",
    "        tf.keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5.5 Train Model\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64  # Increased batch size for better stability\n",
    "\n",
    "# Calculate steps per epoch from the actual number of samples\n",
    "train_samples = 24527  # From earlier data split calculation\n",
    "val_samples = 5255    # From earlier data split calculation\n",
    "\n",
    "train_steps = train_samples // BATCH_SIZE\n",
    "val_steps = val_samples // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Training steps per epoch: {train_steps}\")\n",
    "print(f\"Validation steps per epoch: {val_steps}\")\n",
    "print(f\"Total training samples: {train_samples}\")\n",
    "print(f\"Total validation samples: {val_samples}\\n\")\n",
    "\n",
    "# Create a function to extract inputs and targets from the dataset\n",
    "def prepare_dataset(dataset):\n",
    "    return dataset.map(lambda x: (\n",
    "        {'branch1_input': x['branch1_input'], 'branch2_input': x['branch2_input']},\n",
    "        x['target']\n",
    "    ))\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset_prepared = prepare_dataset(train_dataset)\n",
    "val_dataset_prepared = prepare_dataset(val_dataset)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset_prepared,\n",
    "    validation_data=val_dataset_prepared,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[\n",
    "        lr_scheduler,\n",
    "        early_stopping,\n",
    "        checkpoint,\n",
    "        tensorboard\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5.6 Plot Training History\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot MSE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['mse'], label='Training MSE')\n",
    "plt.plot(history.history['val_mse'], label='Validation MSE')\n",
    "plt.title('Model MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "# Plot RMSE\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['rmse'], label='Training RMSE')\n",
    "plt.plot(history.history['val_rmse'], label='Validation RMSE')\n",
    "plt.title('Model RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5.7 Save Training History\n",
    "# Convert numpy types to Python native types\n",
    "history_dict = {}\n",
    "for key, values in history.history.items():\n",
    "    history_dict[key] = [float(v) for v in values]\n",
    "\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b13bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Visualization of Predictions\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# 7.1 Generate Predictions for a Single Batch\n",
    "print(\"\\nGenerating predictions for a single batch...\")\n",
    "\n",
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_model.h5', \n",
    "                                      custom_objects={\n",
    "                                          'CrossAttention': CrossAttention,\n",
    "                                          'custom_loss': custom_loss\n",
    "                                      })\n",
    "\n",
    "# Prepare test dataset\n",
    "test_dataset_prepared = prepare_dataset(test_dataset)\n",
    "\n",
    "# Get a single batch\n",
    "dataset_iterator = iter(test_dataset_prepared)\n",
    "batch = next(dataset_iterator)\n",
    "inputs, targets = batch\n",
    "predictions = best_model.predict(inputs, verbose=0)\n",
    "\n",
    "# Get timestamps for this batch\n",
    "batch_indices = batch_generator.test_indices[:len(targets)]  # Get indices for this batch\n",
    "batch_timestamps = pd.to_datetime(batch_generator.target_df['DateTime'].iloc[batch_indices])\n",
    "\n",
    "# Create a DataFrame for this batch\n",
    "results_df = pd.DataFrame({\n",
    "    'DateTime': batch_timestamps,\n",
    "    'Be_true': targets[:, 0],\n",
    "    'Bn_true': targets[:, 1],\n",
    "    'Bu_true': targets[:, 2],\n",
    "    'Be_pred': predictions[:, 0],\n",
    "    'Bn_pred': predictions[:, 1],\n",
    "    'Bu_pred': predictions[:, 2]\n",
    "})\n",
    "\n",
    "# Get unique days in this batch\n",
    "unique_days = results_df['DateTime'].dt.date.unique()\n",
    "print(f\"\\nFound {len(unique_days)} unique days in this batch\")\n",
    "\n",
    "# Select 3 random days (or all days if less than 3)\n",
    "num_days = min(3, len(unique_days))\n",
    "random_days = np.random.choice(unique_days, size=num_days, replace=False)\n",
    "random_days.sort()  # Sort for chronological order\n",
    "\n",
    "# 7.2 Create Visualization\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Components and their labels\n",
    "components = ['Be', 'Bn', 'Bu']\n",
    "component_labels = ['East', 'North', 'Up']\n",
    "# Create subplots (3 components × number of days)\n",
    "for day_idx, day in enumerate(random_days):\n",
    "    # Get data for this day\n",
    "    day_data = results_df[results_df['DateTime'].dt.date == day]\n",
    "    day_times = day_data['DateTime']\n",
    "    \n",
    "    # Print time range for debugging\n",
    "    print(f\"\\nDay {day}:\")\n",
    "    print(f\"Time range: {day_times.min()} to {day_times.max()}\")\n",
    "    print(f\"Number of samples: {len(day_times)}\")\n",
    "    \n",
    "    # Plot each component\n",
    "    for comp_idx, (comp, label) in enumerate(zip(components, component_labels)):\n",
    "        subplot_idx = day_idx * 3 + comp_idx + 1\n",
    "        plt.subplot(num_days, 3, subplot_idx)\n",
    "        \n",
    "        # Plot true and predicted values\n",
    "        plt.plot(day_times, day_data[f'{comp}_true'], 'b-', label='True', alpha=0.7)\n",
    "        plt.plot(day_times, day_data[f'{comp}_pred'], 'r--', label='Predicted', alpha=0.7)\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title(f'{label} Component - {day}\\n{day_times.min().strftime(\"%H:%M\")} to {day_times.max().strftime(\"%H:%M\")}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Magnetic Field (nT)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format x-axis to show hours\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add RMSE for this day and component\n",
    "        rmse = np.sqrt(np.mean((day_data[f'{comp}_true'] - day_data[f'{comp}_pred'])**2))\n",
    "        plt.text(0.02, 0.98, f'RMSE: {rmse:.2f}', \n",
    "                transform=plt.gca().transAxes, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7.3 Save Sample Predictions\n",
    "sample_results = {\n",
    "    'dates': [str(day) for day in random_days],\n",
    "    'predictions': {\n",
    "        str(day): {\n",
    "            'times': [str(t) for t in day_data['DateTime']],\n",
    "            'true_values': day_data[['Be_true', 'Bn_true', 'Bu_true']].values.tolist(),\n",
    "            'predicted_values': day_data[['Be_pred', 'Bn_pred', 'Bu_pred']].values.tolist()\n",
    "        } for day, day_data in zip(\n",
    "            random_days,\n",
    "            [results_df[results_df['DateTime'].dt.date == day] for day in random_days]\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('sample_predictions.json', 'w') as f:\n",
    "    json.dump(sample_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a76d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
