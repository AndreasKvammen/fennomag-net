{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5ff5af",
   "metadata": {},
   "source": [
    "# Notebook: SECS Data Extraction and Preprocessing\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook extracts SECS (Spherical Elementary Current Systems) data from CSV files (`Be`, `Bn`, `Bu`) located in a specified data folder. The data is extracted around specified midpoint coordinates (`x_mid`, `y_mid`) with a defined region (`W`, `H`). The processed data arrays (`Be`, `Bn`, `Bu`) are saved into a structured CSV file (`target.csv`) suitable for further analysis or machine learning tasks.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Setup and Path Definitions](#setup-and-path-definitions)\n",
    "2. [Dimension and Region Specification](#dimension-and-region-specification)\n",
    "3. [Utility Functions](#utility-functions)\n",
    "4. [Data Loading and Extraction](#data-loading-and-extraction)\n",
    "5. [Saving Extracted Data](#saving-extracted-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3480c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup and Path Definitions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Define working and output directories\n",
    "data_path = Path('/Users/akv020/Tensorflow/fennomag-net/data/secs/test_mode_2024')\n",
    "output_path = Path('/Users/akv020/Tensorflow/fennomag-net/source/preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcdda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Dimension and Region Specification\n",
    "\n",
    "# Midpoint coordinates for extraction\n",
    "x_mid, y_mid = 10, 10\n",
    "\n",
    "# Region dimensions (radius around midpoint)\n",
    "W, H = 3, 3\n",
    "\n",
    "# Python indexing starts at 0; define slice regions appropriately\n",
    "dx = slice(x_mid - W, x_mid + W + 1)\n",
    "dy = slice(y_mid - H, y_mid + H + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9014900",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "# Utility function to read CSV and extract a region\n",
    "def read_and_extract(file_path, dx, dy):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    data_region = df.values[dx, dy]\n",
    "    return data_region\n",
    "\n",
    "# Function to extract datetime from filename\n",
    "def extract_datetime(filename):\n",
    "    parts = filename.stem.split('_')\n",
    "    return datetime.strptime(parts[1] + parts[2], '%Y%m%d%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13040972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 5270/527040 (1.0%)\n",
      "Processing file 10540/527040 (2.0%)\n",
      "Processing file 15810/527040 (3.0%)\n",
      "Processing file 21080/527040 (4.0%)\n",
      "Processing file 26350/527040 (5.0%)\n",
      "Processing file 31620/527040 (6.0%)\n",
      "Processing file 36890/527040 (7.0%)\n",
      "Processing file 42160/527040 (8.0%)\n",
      "Processing file 47430/527040 (9.0%)\n",
      "Processing file 52700/527040 (10.0%)\n",
      "Processing file 57970/527040 (11.0%)\n",
      "Processing file 63240/527040 (12.0%)\n",
      "Processing file 68510/527040 (13.0%)\n",
      "Processing file 73780/527040 (14.0%)\n",
      "Processing file 79050/527040 (15.0%)\n",
      "Processing file 84320/527040 (16.0%)\n",
      "Processing file 89590/527040 (17.0%)\n",
      "Processing file 94860/527040 (18.0%)\n",
      "Processing file 100130/527040 (19.0%)\n",
      "Processing file 105400/527040 (20.0%)\n",
      "Processing file 110670/527040 (21.0%)\n",
      "Processing file 115940/527040 (22.0%)\n",
      "Processing file 121210/527040 (23.0%)\n",
      "Processing file 126480/527040 (24.0%)\n",
      "Processing file 131750/527040 (25.0%)\n",
      "Processing file 137020/527040 (26.0%)\n",
      "Processing file 142290/527040 (27.0%)\n",
      "Processing file 147560/527040 (28.0%)\n",
      "Processing file 152830/527040 (29.0%)\n",
      "Processing file 158100/527040 (30.0%)\n",
      "Processing file 163370/527040 (31.0%)\n",
      "Processing file 168640/527040 (32.0%)\n",
      "Processing file 173910/527040 (33.0%)\n",
      "Processing file 179180/527040 (34.0%)\n",
      "Processing file 184450/527040 (35.0%)\n",
      "Processing file 189720/527040 (36.0%)\n",
      "Processing file 194990/527040 (37.0%)\n",
      "Processing file 200260/527040 (38.0%)\n",
      "Processing file 205530/527040 (39.0%)\n",
      "Processing file 210800/527040 (40.0%)\n",
      "Processing file 216070/527040 (41.0%)\n",
      "Processing file 221340/527040 (42.0%)\n",
      "Processing file 226610/527040 (43.0%)\n",
      "Processing file 231880/527040 (44.0%)\n",
      "Processing file 237150/527040 (45.0%)\n",
      "Processing file 242420/527040 (46.0%)\n",
      "Processing file 247690/527040 (47.0%)\n",
      "Processing file 252960/527040 (48.0%)\n",
      "Processing file 258230/527040 (49.0%)\n",
      "Processing file 263500/527040 (50.0%)\n",
      "Processing file 268770/527040 (51.0%)\n",
      "Processing file 274040/527040 (52.0%)\n",
      "Processing file 279310/527040 (53.0%)\n",
      "Processing file 284580/527040 (54.0%)\n",
      "Processing file 289850/527040 (55.0%)\n",
      "Processing file 295120/527040 (56.0%)\n",
      "Processing file 300390/527040 (57.0%)\n",
      "Processing file 305660/527040 (58.0%)\n",
      "Processing file 310930/527040 (59.0%)\n",
      "Processing file 316200/527040 (60.0%)\n",
      "Processing file 321470/527040 (61.0%)\n",
      "Processing file 326740/527040 (62.0%)\n",
      "Processing file 332010/527040 (63.0%)\n",
      "Processing file 337280/527040 (64.0%)\n",
      "Processing file 342550/527040 (65.0%)\n",
      "Processing file 347820/527040 (66.0%)\n",
      "Processing file 353090/527040 (67.0%)\n",
      "Processing file 358360/527040 (68.0%)\n",
      "Processing file 363630/527040 (69.0%)\n",
      "Processing file 368900/527040 (70.0%)\n",
      "Processing file 374170/527040 (71.0%)\n",
      "Processing file 379440/527040 (72.0%)\n",
      "Processing file 384710/527040 (73.0%)\n",
      "Processing file 389980/527040 (74.0%)\n",
      "Processing file 395250/527040 (75.0%)\n",
      "Processing file 400520/527040 (76.0%)\n",
      "Processing file 405790/527040 (77.0%)\n",
      "Processing file 411060/527040 (78.0%)\n",
      "Processing file 416330/527040 (79.0%)\n",
      "Processing file 421600/527040 (80.0%)\n",
      "Processing file 426870/527040 (81.0%)\n",
      "Processing file 432140/527040 (82.0%)\n",
      "Processing file 437410/527040 (83.0%)\n",
      "Processing file 442680/527040 (84.0%)\n",
      "Processing file 447950/527040 (85.0%)\n",
      "Processing file 453220/527040 (86.0%)\n",
      "Processing file 458490/527040 (87.0%)\n",
      "Processing file 463760/527040 (88.0%)\n",
      "Processing file 469030/527040 (89.0%)\n",
      "Processing file 474300/527040 (90.0%)\n",
      "Processing file 479570/527040 (91.0%)\n",
      "Processing file 484840/527040 (92.0%)\n",
      "Processing file 490110/527040 (93.0%)\n",
      "Processing file 495380/527040 (94.0%)\n",
      "Processing file 500650/527040 (95.0%)\n",
      "Processing file 505920/527040 (96.0%)\n",
      "Processing file 511190/527040 (97.0%)\n",
      "Processing file 516460/527040 (98.0%)\n",
      "Processing file 521730/527040 (99.0%)\n",
      "Processing file 527000/527040 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "## 4. Data Loading and Extraction\n",
    "\n",
    "# Get sorted list of files\n",
    "Be_files = sorted((data_path / 'Be').glob('*.csv'))\n",
    "Bn_files = sorted((data_path / 'Bn').glob('*.csv'))\n",
    "Bu_files = sorted((data_path / 'Bu').glob('*.csv'))\n",
    "\n",
    "# Initialize a list to store rows of data\n",
    "data_rows = []\n",
    "total_files = len(Be_files)\n",
    "progress_step = max(total_files // 100, 1)\n",
    "\n",
    "# Process files (assuming identical timestamps for Be, Bn, Bu)\n",
    "for idx, (be_file, bn_file, bu_file) in enumerate(zip(Be_files, Bn_files, Bu_files), 1):\n",
    "    if idx % progress_step == 0:\n",
    "        print(f\"Processing file {idx}/{total_files} ({(idx/total_files)*100:.1f}%)\")\n",
    "\n",
    "    timestamp = extract_datetime(be_file)\n",
    "\n",
    "    be_region = read_and_extract(be_file, dx, dy).flatten()\n",
    "    bn_region = read_and_extract(bn_file, dx, dy).flatten()\n",
    "    bu_region = read_and_extract(bu_file, dx, dy).flatten()\n",
    "\n",
    "    row = [timestamp] + be_region.tolist() + bn_region.tolist() + bu_region.tolist()\n",
    "    data_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f00a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and saving to 'target.csv' completed.\n"
     ]
    }
   ],
   "source": [
    "# Define column names based on region dimensions\n",
    "region_indices = [(i, j) for i in range(-W, W+1) for j in range(-H, H+1)]\n",
    "columns = ['DateTime']\n",
    "\n",
    "# Add component names with offsets to columns\n",
    "for comp in ['Be', 'Bn', 'Bu']:\n",
    "    for idx in region_indices:\n",
    "        columns.append(f\"{comp}_{idx[0]}_{idx[1]}\")\n",
    "\n",
    "# Create DataFrame and save as CSV with single precision\n",
    "result_df = pd.DataFrame(data_rows, columns=columns)\n",
    "result_df.to_csv(data_path / 'target_2D.csv', index=False, float_format='%.6g')\n",
    "\n",
    "print(\"Data extraction and saving to 'target.csv' completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14f1b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(7, 14, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc11ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(7, 14, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7eef22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akv020/Tensorflow/fennomag-net/source/preprocess'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b39913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual structure\n",
    "secs_data = np.array(shape=(n_timestamps, 21, 21, 3), dtype=np.float32)\n",
    "# Where:\n",
    "# - n_timestamps: number of 1-minute intervals in the dataset\n",
    "# - 21x21: spatial grid\n",
    "# - 3: magnetic field components (Be, Bn, Bu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
